/*
 * DDlog implementation of Antrea controller logic (see networkpolicy_controller.go).
 */

import intern
import k8spolicy as k8s
import types
import klog as klog
import uuid as uuid

/* In DDlog, we split the AppliedToGroup struct into three relations:
 * AppliedToGroupDescrInternal: stores group name, id, and GroupSelector
 * AppliedToGroupPodsByNode: stores the node-to-pods mapping.
 * AppliedToGroupSpan: group span (nodes where the group must be sent).
 */

relation &AppliedToGroupDescrInternal (
    // UID is generated from the hash value of GroupSelector.NormalizedName.
    uid: UUID,
    // Name of this group, currently it's same as UID.
    name: string,
    // Namespace name.
    namespace: k8s::NSName,
    // Selector describes how the group selects pods.
    selector: GroupSelector
)

output relation AppliedToGroupDescr (
    name: string,
    selector: GroupSelector
)

AppliedToGroupDescr(name, selector) :-
    &AppliedToGroupDescrInternal(.name = name, .selector = selector).


// PodsByNode is a mapping from nodeName to a set of Pods on the Node.
output relation AppliedToGroupPodsByNode(appliedToGroup: UUID, node: string, pods: Set<k8s::PodReference>)

// AppliedToGroupSpan: set of node names that this AddressGroup should be sent to.
output relation AppliedToGroupSpan(appliedToGroup: UUID, span: string)

/*
output relation AppliedToGroup (
    descr: Ref<AppliedToGroupDescrInternal>,
    podsByNode: Map<string, Set<k8s::PodReference>>,
    span: Set<string>
)

AppliedToGroup(descr, podsByNode, span) :-
    descr in &AppliedToGroupDescrInternal(),
    AppliedToGroupPodsByNode(descr.uid, podsByNode),
    AppliedToGroupSpan(descr.uid, span).
*/

/* In DDlog, we split the AddressGroup struct into three relations:
 * AddressGroupDescrInternal: stores group name, id, and GroupSelector
 * AddressGroupAddress: stores addresses of nodes that match the group selector.
 * AddressGroupSpan: group span (nodes where the group must be sent).
 */

// AddressGroupDescrInternal describes a set of addresses used as source or destination of Network Policy rules.
relation &AddressGroupDescrInternal (
    // UID is generated from the hash value of GroupSelector.NormalizedName.
    uid: UUID,
    // Name of this group, currently it's same as UID.
    name: string,
    // Namespace name.
    namespace: k8s::NSName,
    // Selector describes how the group selects pods to get their addresses.
    selector: GroupSelector
)

output relation AddressGroupDescr (
    name: string,
    selector: GroupSelector
)

AddressGroupDescr(name, selector) :-
    &AddressGroupDescrInternal(.name = name, .selector = selector).

// Addresses is a set of IP addresses selected by this group.
output relation AddressGroupAddress(addressGroup: UUID, address: string)

/*
// Aggregate all addresses for an address group in one record.
relation AddressGroupAddresses(addressGroup: UUID, addresses: Set<string>)
AddressGroupAddresses(addressGroup, addresses) :-
    AddressGroupAddress(addressGroup, address),
    var addresses = Aggregate((addressGroup), group2set(address)).

// Handle the case when the group is empty.
AddressGroupAddresses(gid, set_empty()) :-
    &AddressGroupDescrInternal(.uid = gid),
    not AddressGroupAddress(.addressGroup = gid).
*/

// AddressGroupSpan: set of node names that this AddressGroup should be sent to.
output relation AddressGroupSpan(addressGroup: UUID, node: string)

/*
output relation AddressGroup (
    descr: Ref<AddressGroupDescrInternal>,
    addresses: Set<string>,
    span: Set<string>
)

AddressGroup(descr, addresses, span) :-
    descr in &AddressGroupDescrInternal(),
    AddressGroupAddresses(descr.uid, addresses),
    AddressGroupSpan(descr.uid, span).
*/

// NetworkPolicy describes what network traffic is allowed for a set of Pods.
output relation &NetworkPolicyDescr (
    // UID of the original K8s Network Policy.
    uid: k8s::UID,
    // Name of the original K8s Network Policy.
    name: string,
    // Namespace of the original K8s Network Policy.
    namespace: k8s::NSName,
    // Rules is a list of rules to be applied to the selected Pods.
    rules: Vec<NetworkPolicyRule>,
    // AppliedToGroups is a list of names of AppliedToGroups to which this policy applies.
    appliedToGroups: Vec<UUID>
)

// NetworkPolicySpanInternal: set of node names that this NetworkPolicyDescr should be sent to.
relation NetworkPolicySpanInternal(
    policy: Ref<NetworkPolicyDescr>,
    //namespace: k8s::NSName,
    //policy_name: string,
    //policy: k8s::UID,
    node: string)

output relation NetworkPolicySpan(
    namespace: k8s::NSName,
    policy_name: string,
    policy: k8s::UID,
    node: string)

NetworkPolicySpan(policy.namespace, policy.name, policy.uid, node) :-
    NetworkPolicySpanInternal(policy, node).


/*
output relation NetworkPolicy(
    descr: NetworkPolicyDescr,
    span: Set<string>)

NetworkPolicy(descr, span) :-
    descr in NetworkPolicyDescr(),
    NetworkPolicySpanInternal(descr.uid, span).
*/

// createAppliedToGroup creates an AppliedToGroupDescrInternal object.
function createAppliedToGroup(np: k8s::NetworkPolicy): AppliedToGroupDescrInternal =
{
    //klog::info("createAppliedToGroup ${np.name}");
    var groupSelector = toGroupSelector(np.namespace, Some{np.spec.podSelector}, None);
    var appliedToGroupUID = getNormalizedUID(groupSelector.normalizedName);
    // Construct a new AppliedToGroupDescrInternal.
    AppliedToGroupDescrInternal {
        .uid        = appliedToGroupUID,
        .name       = uuid::to_hyphenated_lower(uuid::from_u128(appliedToGroupUID)),
        .namespace  = np.namespace,
        .selector   = groupSelector
    }
}

function toAntreaPeer(peers: Vec<k8s::NetworkPolicyPeer>, np: k8s::NetworkPolicy): (NetworkPolicyPeer, Vec<Ref<AddressGroupDescrInternal>>) =
{
    // Empty NetworkPolicyPeer is supposed to match all addresses.
    // See https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-allow-all-ingress-traffic.
    // It's treated as an IPBlock "0.0.0.0/0".
    if (vec_is_empty(peers)) {
        (matchAllPeer(), vec_empty())
    } else {
        var ipBlocks: Vec<IPBlock> = vec_empty();
        var addressGroupIds: Vec<UUID> = vec_empty();
        var addressGroups: Vec<Ref<AddressGroupDescrInternal>> = vec_empty();

        for (peer in peers) {
            // A networking.NetworkPolicyPeer will either have an IPBlock or a
            // podSelector and/or namespaceSelector set.
            match (peer.ipBlock) {
                Some{peerIPBlock} -> {
                    match (toAntreaIPBlock(peerIPBlock)) {
                        Err{err} -> {
                            klog::error("Failure processing NetworkPolicy ${istring_str(np.namespace)}/${np.name} IPBlock ${peerIPBlock}: ${err}");
                            ()
                        },
                        Ok{ipBlock} -> {
                            vec_push(ipBlocks, ipBlock)
                        }
                    }
                },
                None -> {
                    var addressGroup = ref_new(createAddressGroup(peer, np));
                    vec_push(addressGroups, addressGroup);
                    vec_push(addressGroupIds, addressGroup.uid)
                }
            }
        };
        var nppeer = NetworkPolicyPeer {
            .addressGroups  = addressGroupIds,
            .ipBlocks       = ipBlocks
        };
        (nppeer, addressGroups)
    }
}

// toAntreaProtocol converts a v1.Protocol object to an Antrea Protocol object.
function toAntreaProtocol(npProtocol: Option<k8s::Protocol>): Protocol =
{
    // If Protocol is unset, it must default to TCP protocol.
    match (npProtocol) {
        None -> protocolTCP(),
        Some{proto} -> proto
    }
}

// toAntreaServices converts a networkingv1.NetworkPolicyPort object to an
// Antrea Service object.
function toAntreaServices(npPorts: Vec<k8s::NetworkPolicyPort>): Vec<Service> =
{
    var antreaServices: Vec<Service> = vec_empty();
    for (npPort in npPorts) {
        var antreaService = Service {
            .protocol   = toAntreaProtocol(npPort.protocol),
            .port       = match (npPort.port) {
                None    -> None: Option<signed<32>>,
                // TODO(abhiraut): Retrieve ports for named ports.
                Some{p} -> Some{ k8s::intOrStringIntValue(p) }
            }
        };
        vec_push(antreaServices, antreaService)
    };
    antreaServices
}

// toAntreaIPBlock converts a networkingv1.IPBlock to an Antrea IPBlock.
function toAntreaIPBlock(ipBlock: k8s::IPBlock): Result<IPBlock,string> =
{
    // Convert the allowed IPBlock to networkpolicy.IPNet.
    match (cidrStrToIPNet(ipBlock.cidr)) {
        Err{err} -> Err{err},
        Ok{ipNet} -> {
            var exceptNets: Vec<IPNet> = vec_empty();
            for (exc in ipBlock.except) {
                // Convert the except IPBlock to networkpolicy.IPNet.
                match (cidrStrToIPNet(exc)) {
                    Err{e} -> {
                        return Err{e}
                    },
                    Ok{exceptNet} -> {
                        vec_push(exceptNets, exceptNet)
                    }
                }
            };
            Ok { IPBlock { .cidr   = ipNet, .except = exceptNets } }
        }
    }
}

// createAddressGroup creates an AddressGroupDescrInternal object corresponding to a
// NetworkPolicyPeer object in NetworkPolicyRule. This function simply
// creates the object without actually populating the PodAddresses as the
// affected Pods are calculated during sync process.
function createAddressGroup(peer: k8s::NetworkPolicyPeer, np: k8s::NetworkPolicy): AddressGroupDescrInternal =
{
    var groupSelector = toGroupSelector(np.namespace, peer.podSelector, peer.namespaceSelector);
    var normalizedUID = getNormalizedUID(groupSelector.normalizedName);

    // Create an AddressGroupDescrInternal object per Peer object.
    AddressGroupDescrInternal{
        .uid        = normalizedUID,
        .name       = uuid::to_hyphenated_lower(uuid::from_u128(normalizedUID)),
        .namespace  = np.namespace,
        .selector   = groupSelector
    }
}

// processNetworkPolicy creates an internal NetworkPolicy instance corresponding
// to the networkingv1.NetworkPolicy object. This method does not commit the
// internal NetworkPolicy in store, instead returns an instance to the caller
// wherein, it will be either stored as a new Object in case of ADD event or
// modified and store the updated instance, in case of an UPDATE event.
function processNetworkPolicy(np: k8s::NetworkPolicy): (Ref<NetworkPolicyDescr>, Ref<AppliedToGroupDescrInternal>, Vec<Ref<AddressGroupDescrInternal>>) =
{
    var appliedToGroup = ref_new(createAppliedToGroup(np));
    var rules: Vec<NetworkPolicyRule> = vec_with_capacity(vec_len(np.spec.ingress) + vec_len(np.spec.egress));
    var addressGroups: Vec<Ref<AddressGroupDescrInternal>> = vec_empty();
    var ingressRuleExists = false;
    var egressRuleExists  = false;

    // Compute NetworkPolicyRule for Ingress Rule.
    for (ingressRule in np.spec.ingress) {
        ingressRuleExists = true;
        (var from, var agroups) = toAntreaPeer(ingressRule.from, np);
        vec_append(addressGroups, agroups);
        vec_push(rules, NetworkPolicyRule{
            .direction = DirectionIn,
            .from      = from,
            .to        = networkPolicyPeerEmpty(),
            .services  = toAntreaServices(ingressRule.ports)
        })
    };
    // Compute NetworkPolicyRule for Egress Rule.
    for (egressRule in np.spec.egress) {
        egressRuleExists = true;
        (var to, var agroups) = toAntreaPeer(egressRule.to, np);
        vec_append(addressGroups, agroups);
        vec_push(rules, NetworkPolicyRule {
            .direction = DirectionOut,
            .from      = networkPolicyPeerEmpty(),
            .to        = to,
            .services  = toAntreaServices(egressRule.ports)
        })
    };

    // Traffic in a direction must be isolated if Spec.PolicyTypes specify it explicitly.
    var ingressIsolated = false;
    var egressIsolated = false;
    for (policyType in np.spec.policyTypes) {
        match (policyType) {
            k8s::PolicyTypeIngress -> ingressIsolated = true,
            k8s::PolicyTypeEgress  -> egressIsolated = true
        }
    };

    // If ingress isolation is specified explicitly and there's no ingress rule, append a deny-all ingress rule.
    // See https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic
    if (ingressIsolated and (not ingressRuleExists)) {
        vec_push(rules, denyAllIngressRule())
    };
    // If egress isolation is specified explicitly and there's no egress rule, append a deny-all egress rule.
    // See https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-egress-traffic
    if (egressIsolated and (not egressRuleExists)) {
        vec_push(rules, denyAllEgressRule())
    };

    var policy = ref_new(NetworkPolicyDescr {
        .name               = np.name,
        .namespace          = np.namespace,
        .uid                = np.uid,
        .appliedToGroups    = vec_singleton(appliedToGroup.uid),
        .rules              = rules
    });

    (policy, appliedToGroup, addressGroups)
}

// Intermediate relation that stores policy and all associated address groups.
// This relation does not get indexed and therefore should not take any memory.
relation NetworkPolicyExt(
    policy: Ref<NetworkPolicyDescr>,
    addressGroups: Vec<Ref<AddressGroupDescrInternal>>
)

NetworkPolicyExt(internalPolicy, addrGroups),
AppliedToGroupDescrInternal[appliedTo] :-
    k8s::NetworkPolicy[policy],
    (var internalPolicy, var appliedTo, var addrGroups) = processNetworkPolicy(policy).

NetworkPolicyDescr[policy] :- NetworkPolicyExt(policy, _).

/*
 * The following rules match pods to pod selectors and store results in the
 * `PodMatchesSelector` relation.
 */

// `PodLabel` - flattens pod labels in a separate relation with one label per
// record, so we can join it with other relations on label name and value.
relation PodLabel(pod: Ref<k8s::Pod>, label: string, val: string)
PodLabel(pod, label.0, label.1) :-
    pod in &k8s::Pod(),
    var label = FlatMap(pod.labels).

// Pod selectors collected from AppliedToGroup's and AddressGroup's.
relation PodSelector(uid: UUID, namespace: k8s::NSName, requirements: Requirements)
PodSelector(descr.uid, descr.namespace, labelSelectorToRequirements(selector)) :-
    descr in &AppliedToGroupDescrInternal(),
    Some{var selector} = descr.selector.podSelector.
PodSelector(descr.uid, descr.namespace, labelSelectorToRequirements(selector)) :-
    descr in &AddressGroupDescrInternal(),
    Some{var selector} = descr.selector.podSelector.

// Positive requirements extracted from all pod selectors in the system,
// including `AppliedToGroupDescrInternal` selectors and `AddressGroupDescrInternal` selectors.
relation PositiveRequirement(requirement: Ref<k8s::LabelSelectorRequirement>, namespace: k8s::NSName)
PositiveRequirement(req, namespace) :-
    PodSelector(.requirements = requirements, .namespace = namespace),
    var req = FlatMap(requirements.0).

// Negative requirements extracted from all pod selectors in the system,
// including `AppliedToGroupDescrInternal` selectors and `AddressGroupDescrInternal` selectors.
relation NegativeRequirement(requirement: Ref<k8s::LabelSelectorRequirement>, namespace: k8s::NSName)
NegativeRequirement(req, namespace) :-
    PodSelector(.requirements = requirements, .namespace = namespace),
    var req = FlatMap(requirements.1).

// `PodSatisfiesRequirement`: For positive requirements (`In` and `Exists`),
// tracks all pods that satisfy the requirement.
relation PodSatisfiesRequirement(
    pod: Ref<k8s::Pod>,
    requirement: Ref<k8s::LabelSelectorRequirement>
)

PodSatisfiesRequirement(pod, requirement) :-
    PositiveRequirement(requirement@ &k8s::LabelSelectorRequirement{.operator = k8s::LabelSelectorOpExists, .reqkey = label}, namespace),
    PodLabel(.pod = pod@ &k8s::Pod{.namespace = namespace}, .label = label).

PodSatisfiesRequirement(pod, requirement) :-
    PositiveRequirement(requirement@ &k8s::LabelSelectorRequirement{.operator = k8s::LabelSelectorOpIn, .reqkey = label, .values = values}, namespace),
    var value = FlatMap(values),
    PodLabel(.pod = pod@ &k8s::Pod{.namespace = namespace}, .label = label, .val = value).

// `PodViolatesRequirement`: For negative requirements (`NotIn` and `DoesNotExist`),
// tracks all pods that violate the requirement.
relation PodViolatesRequirement(
    pod: Ref<k8s::Pod>,
    requirement: Ref<k8s::LabelSelectorRequirement>
)

PodViolatesRequirement(pod, requirement) :-
    NegativeRequirement(requirement@ &k8s::LabelSelectorRequirement{.operator = k8s::LabelSelectorOpDoesNotExist, .reqkey = label}, namespace),
    PodLabel(.pod = pod@ &k8s::Pod{.namespace = namespace}, .label = label).

PodViolatesRequirement(pod, requirement) :-
    NegativeRequirement(requirement@ &k8s::LabelSelectorRequirement{.operator = k8s::LabelSelectorOpNotIn, .reqkey = label, .values = values}, namespace),
    var value = FlatMap(values),
    PodLabel(.pod = pod@ &k8s::Pod{.namespace = namespace}, .label = label, .val = value).

// `PodMatchesPositiveRequirements`: For each selector in `PodSelector`,
// tracks pods that satisfy the subset of positive requirements.
relation PodMatchesPositiveRequirements(selector: UUID, pod: Ref<k8s::Pod>)

PodMatchesPositiveRequirements(sel_uid, pod) :-
    PodSelector(sel_uid, namespace, (positive_requirements, _)),
    var expected_matches = vec_len(positive_requirements),
    var req = FlatMap(positive_requirements),
    PodSatisfiesRequirement(pod@&k8s::Pod{.namespace = namespace}, req),
    var num_matches = Aggregate((sel_uid, pod, expected_matches), group_count(())),
    num_matches == expected_matches.

// `PodViolatesNegativeRequirements`: For each selector in `PodSelector`,
// tracks pods that violate at least one of its of negative requirements.
relation PodViolatesNegativeRequirements(selector: UUID, pod: Ref<k8s::Pod>)

PodViolatesNegativeRequirements(sel_uid, pod) :-
    PodSelector(sel_uid, namespace , (_, negative_requirements)),
    var req = FlatMap(negative_requirements),
    PodViolatesRequirement(pod@&k8s::Pod{.namespace = namespace}, req).

// `PodMatchesSelector`: For each selector in the `PodSelector` table,
// tracks pods that satisfy the selector.
relation PodMatchesSelector(uid: UUID, pod: Ref<k8s::Pod>)

// Case 1: non-empty positive requirements lists.
PodMatchesSelector(sel_uid, pod) :-
    PodMatchesPositiveRequirements(sel_uid, pod),
    not PodViolatesNegativeRequirements(sel_uid, pod).

// Case 2: empty positive requirements list.
PodMatchesSelector(sel_uid, pod) :-
    PodSelector(sel_uid, namespace, (vec_empty(), _)),
    pod in &k8s::Pod(.namespace = namespace),
    not PodViolatesNegativeRequirements(sel_uid, pod).

/************************************/

AddressGroupDescrInternal[group] :-
    NetworkPolicyExt(_, groups),
    var group = FlatMap(groups).

AddressGroupAddress(addressGroup.uid, pod.status.podIP) :-
    // Namespace presence indicates Pods must be selected from the same Namespace.
    addressGroup in &AddressGroupDescrInternal(.selector = GroupSelector{.nsSelector = NSSelectorNS{ns}}),
    PodMatchesSelector(addressGroup.uid, pod@ &k8s::Pod{.namespace = ns}),
    pod.status.podIP != "".

AddressGroupAddress(addressGroup.uid, pod.status.podIP) :-
    // Pods must be selected from Namespaces matching nsSelector.
    addressGroup in &AddressGroupDescrInternal(.selector = GroupSelector{.nsSelector = NSSelectorLS{namespaceSelector}}),
    namespace in k8s::Namespace(),
    k8s::labelSelectorMatches(Some{namespaceSelector}, namespace.labels),
    PodMatchesSelector(addressGroup.uid, pod@ &k8s::Pod{.namespace = namespace.name}),
    pod.status.podIP != "".

AddressGroupAddress(addressGroup.uid, pod.status.podIP) :-
    // Pods must be selected from Namespaces matching nsSelector.
    addressGroup in &AddressGroupDescrInternal(.selector = GroupSelector{.nsSelector = NSSelectorLS{namespaceSelector}, .podSelector = None}),
    namespace in k8s::Namespace(),
    k8s::labelSelectorMatches(Some{namespaceSelector}, namespace.labels),
    pod in &k8s::Pod(.namespace = namespace.name),
    pod.status.podIP != "".

// NetworkPolicyAddressGroup: stores all address groups that a network policy references;
// used to compute AddressGroupSpan.
//relation NetworkPolicyAddressGroup(np: k8s::UID, addressGroup: UUID)

/*NetworkPolicyAddressGroup(np, addressGroup) :-
    &NetworkPolicyDescr(.uid = np, .rules = rules),
    var addressGroups = {
        var addressGroups: Vec<UUID> = vec_empty();
        for (rule in rules) {
            vec_append(addressGroups, rule.from.addressGroups);
            vec_append(addressGroups, rule.to.addressGroups)
        };
        addressGroups
    },
    var addressGroup = FlatMap(addressGroups).
*/

AddressGroupSpan(addressGroup, node) :-
    //&AddressGroupDescrInternal(.uid = addressGroup),
    // Get all internal NetworkPolicy objects that refers this AddressGroupDescrInternal.
    //NetworkPolicyAddressGroup(np, addressGroup),
    NetworkPolicySpanInternal(.policy = np, .node = node),
    var rule = FlatMap(np.rules),
    var addressGroup = FlatMap(rule.from.addressGroups).

AddressGroupSpan(addressGroup, node) :-
    //&AddressGroupDescrInternal(.uid = addressGroup),
    // Get all internal NetworkPolicy objects that refers this AddressGroupDescrInternal.
    //NetworkPolicyAddressGroup(np, addressGroup),
    NetworkPolicySpanInternal(.policy = np, .node = node),
    var rule = FlatMap(np.rules),
    var addressGroup = FlatMap(rule.to.addressGroups).


//, var span = Aggregate((addressGroup), group_set_unions(npSpan)).

// The following rule is unnecessary, as by construction each group is derived from at
// least one network policy.

// Handle the case when the address group is not refered to by any NetworkPolicy
// objects.
//AddressGroupSpan(addressGroup, set_empty()) :-
//    &AddressGroupDescrInternal(.uid = addressGroup),
//    not NetworkPolicyAddressGroup(.addressGroup = addressGroup).

// AppliedToGroupPod: pods that belong to a group along with nodes
// where the pod is scheduled;
// used to compute AppliedToGroupPodsByNode and AppliedToGroupSpan.
relation AppliedToGroupPod(appliedToGroup: UUID, pod: k8s::PodReference, nodeName: string)

AppliedToGroupPod(appliedToGroup, k8s::PodReference{pod.name, pod.namespace}, pod.spec.nodeName) :-
    // TODO(leonid): Is it correct that we only consider selectors with namespace name set?
    &AppliedToGroupDescrInternal(.uid = appliedToGroup,
                         .selector = GroupSelector{.nsSelector = NSSelectorNS{namespace}, .podSelector = Some{}}),
    // Retrieve all Pods matching the podSelector.
    PodMatchesSelector(appliedToGroup, pod @ &k8s::Pod{.namespace = namespace}),
    //pod in &k8s::Pod(.uid = pod_uid, .namespace = namespace),
    // No need to process Pod when it's not scheduled.
    pod.spec.nodeName != "".

AppliedToGroupPodsByNode(appliedToGroup, nodeName, pods) :-
    AppliedToGroupPod(appliedToGroup, pod, nodeName),
    var pods = Aggregate((appliedToGroup, nodeName), group_to_set(pod)).
    //var podsByNode = Aggregate((appliedToGroup), group2map((nodeName, podsOnNode))).

AppliedToGroupSpan(appliedToGroup, nodeName) :-
    AppliedToGroupPod(appliedToGroup, _, nodeName).
//    var span = Aggregate((appliedToGroup), group2set(nodeName)).

// Handle the case when there are no pods in the group.
//AppliedToGroupSpan(uid, set_empty()),
//AppliedToGroupPodsByNode(uid, map_empty()) :-
//    &AppliedToGroupDescrInternal(.uid = uid),
//    not AppliedToGroupPod(.appliedToGroup = uid).

NetworkPolicySpanInternal(descr, node) :-
    descr in &NetworkPolicyDescr(),
    var appliedToGroup = FlatMap(descr.appliedToGroups),
    AppliedToGroupSpan(appliedToGroup, node).

//,
//    var span = Aggregate((policy), group_set_unions(groupSpan)).

// Handle the case when `apliedToGroups` is empty.
/*NetworkPolicySpanInternal(policy, set_empty()) :-
    NetworkPolicyDescr(.uid = policy, .appliedToGroups = vec_empty()).*/
